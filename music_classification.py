import pandas as pd
import numpy as np
import torch
import os
import glob
import librosa
import librosa.display
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch.nn.functional as F
from tqdm import tqdm
import time
import random
import optuna
from optuna.integration import PyTorchLightningPruningCallback
from sklearn.metrics import accuracy_score, classification_report
warnings.filterwarnings('ignore')

def load_all_features():
    """åŠ è½½æ‰€æœ‰ç‰¹å¾æ–‡ä»¶ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼‰"""
    print("ğŸ“‚ åŠ è½½æ‰€æœ‰ç‰¹å¾æ–‡ä»¶...")

    feature_files = glob.glob(os.path.join(FEATURE_PATH, 'features_*.npz'))
    if not feature_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾æ–‡ä»¶ï¼")
        return None, None, None
    
    # åˆå§‹åŒ–ç©ºåˆ—è¡¨ï¼ˆé¢„åˆ†é…å†…å­˜æ›´é«˜æ•ˆï¼‰
    all_features = []
    all_labels = []
    all_track_ids = []
    
    for file in feature_files:
        print(f"   åŠ è½½: {file}")
        with np.load(file) as data:  # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿æ–‡ä»¶åŠæ—¶å…³é—­
            # å…³é”®ä¿®æ”¹1ï¼šç«‹å³è½¬æ¢ä¸ºfloat32å‡å°‘å†…å­˜
            all_features.append(data['features'].astype(np.float32))
            all_labels.append(data['labels'])
            all_track_ids.append(data['track_ids'])
    
    # å…³é”®ä¿®æ”¹2ï¼šåˆ†æ­¥åˆå¹¶+åŠæ—¶é‡Šæ”¾å†…å­˜
    features = np.concatenate(all_features, axis=0)
    del all_features  # ç«‹å³åˆ é™¤ä¸´æ—¶å˜é‡
    labels = np.concatenate(all_labels, axis=0)
    track_ids = np.concatenate(all_track_ids, axis=0)
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    import gc
    gc.collect()
    
    print(f"âœ… åŠ è½½å®Œæˆ! æ€»æ ·æœ¬æ•°: {len(features)}, ç‰¹å¾ç»´åº¦: {features.shape}")
    return features, labels, track_ids

class MusicDataset(Dataset):
    def __init__(self, features, labels, transform=None):
        self.features = features
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        feature = self.features[idx]
        label = self.labels[idx]
        
        # è½¬æ¢ä¸ºå¼ é‡
        feature = torch.FloatTensor(feature).unsqueeze(0)  # æ·»åŠ é€šé“ç»´åº¦
        label = torch.LongTensor([label])[0]
        
        if self.transform:
            feature = self.transform(feature)
        
        return feature, label

def create_data_splits(features, labels, test_size=0.2, val_size=0.1, random_state=42):
    """
    åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†
    """
    print("ğŸ”„ åˆ’åˆ†æ•°æ®é›†...")
    
    # é¦–å…ˆåˆ†ç¦»å‡ºæµ‹è¯•é›†
    X_temp, X_test, y_temp, y_test = train_test_split(
        features, labels, test_size=test_size, 
        random_state=random_state, stratify=labels
    )
    
    # ä»å‰©ä½™æ•°æ®ä¸­åˆ†ç¦»å‡ºéªŒè¯é›†
    val_size_adjusted = val_size / (1 - test_size)  # è°ƒæ•´éªŒè¯é›†æ¯”ä¾‹
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size_adjusted, 
        random_state=random_state, stratify=y_temp
    )
    
    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬ ({len(X_train)/len(features)*100:.1f}%)")
    print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬ ({len(X_val)/len(features)*100:.1f}%)")
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬ ({len(X_test)/len(features)*100:.1f}%)")
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    print(f"\nğŸ“Š å„é›†åˆçš„æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"   è®­ç»ƒé›†: {np.bincount(y_train)}")
    print(f"   éªŒè¯é›†: {np.bincount(y_val)}")
    print(f"   æµ‹è¯•é›†: {np.bincount(y_test)}")
    
    return X_train, X_val, X_test, y_train, y_val, y_test

# æ•°æ®å¢å¼º
class AudioTransform:
    def __init__(self, noise_factor=0.005, time_shift_factor=0.1):
        self.noise_factor = noise_factor
        self.time_shift_factor = time_shift_factor
    
    def __call__(self, x):
        # æ·»åŠ å™ªå£°
        if random.random() > 0.5:
            noise = torch.randn_like(x) * self.noise_factor
            x = x + noise
        
        # æ—¶é—´åç§»
        if random.random() > 0.5:
            shift = int(x.shape[-1] * self.time_shift_factor * (random.random() - 0.5))
            if shift != 0:
                if shift > 0:
                    x = torch.cat([x[..., shift:], torch.zeros_like(x[..., :shift])], dim=-1)
                else:
                    x = torch.cat([torch.zeros_like(x[..., :abs(shift)]), x[..., :shift]], dim=-1)
        
        return x


    
class CNN(nn.Module):
    def __init__(self, num_classes=8):
        super(CNN, self).__init__()
        
        self.features = nn.Sequential(
            # ç¬¬ä¸€å±‚
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
            
            # ç¬¬äºŒå±‚
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
            
            # ç¬¬ä¸‰å±‚
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
            
            # ç¬¬å››å±‚
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
        )
        
        # è‡ªé€‚åº”æ± åŒ–
        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256 * 4 * 4, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x




def train_epoch(model, dataloader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        with torch.no_grad():  
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

def validate_epoch(model, dataloader, criterion, device):
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in dataloader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

def plot_training_history(train_losses, val_losses, train_accs, val_accs):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # æŸå¤±æ›²çº¿
    ax1.plot(train_losses, label='Training Loss', color='blue')
    ax1.plot(val_losses, label='Validation Loss', color='red')
    ax1.set_title('Loss vs Epoch')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(train_accs, label='Training Accuracy', color='blue')
    ax2.plot(val_accs, label='Validation Accuracy', color='red')
    ax2.set_title('Accuracy vs Epoch')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1

        if self.counter >= self.patience:
            if self.restore_best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False

    def save_checkpoint(self, model):
        self.best_weights = model.state_dict().copy()
        
        
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, 
                num_epochs=50, device='cuda', patience=10):
    """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch...")
    
    # åˆå§‹åŒ–è®°å½•
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    
    # æ—©åœæœºåˆ¶
    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)
    
    best_val_acc = 0.0
    start_time = time.time()
    
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        print("-" * 30)
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        
        # éªŒè¯
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        
        # è°ƒæ•´å­¦ä¹ ç‡
        if scheduler:
            scheduler.step(val_loss)
        
        # æ‰“å°ç»“æœ
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_music_model.pth')
            print(f"ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_loss, model):
            print(f"â¹ï¸  æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} ä¸ªepochåœæ­¢è®­ç»ƒ")
            break
    
    training_time = time.time() - start_time
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"â±ï¸  è®­ç»ƒæ—¶é—´: {training_time/60:.2f} åˆ†é’Ÿ")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    
    return train_losses, val_losses, train_accs, val_accs




class ConfigurableCNN(nn.Module):
    """å¯é…ç½®çš„CNNæ¨¡å‹ï¼Œç”¨äºè¶…å‚æ•°è°ƒä¼˜"""
    
    def __init__(self, trial, num_classes=8):
        super(ConfigurableCNN, self).__init__()
        
        # é€šè¿‡trialå¯¹è±¡è·å–è¶…å‚æ•°
        self.n_layers = trial.suggest_int('n_layers', 2, 5)
        
        layers = []
        in_channels = 1
        
        for i in range(self.n_layers):
            # æ¯å±‚çš„é€šé“æ•°
            out_channels = trial.suggest_categorical(f'n_units_l{i}', [16, 32, 64, 128, 256])
            
            # å·ç§¯æ ¸å¤§å°
            kernel_size = trial.suggest_categorical(f'kernel_size_l{i}', [3, 5])
            
            # Dropoutæ¦‚ç‡
            dropout_rate = trial.suggest_float(f'dropout_l{i}', 0.1, 0.5)
            
            # æ„å»ºå·ç§¯å—
            layers.extend([
                nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(),
                nn.MaxPool2d(2, 2),
                nn.Dropout2d(dropout_rate)
            ])
            
            in_channels = out_channels
        
        self.features = nn.Sequential(*layers)
        
        # è‡ªé€‚åº”æ± åŒ–
        pool_size = trial.suggest_categorical('pool_size', [2, 4, 8])
        self.adaptive_pool = nn.AdaptiveAvgPool2d((pool_size, pool_size))
        
        # åˆ†ç±»å™¨
        fc_input_size = in_channels * pool_size * pool_size
        hidden_size = trial.suggest_categorical('fc_hidden_size', [128, 256, 512])
        final_dropout = trial.suggest_float('final_dropout', 0.2, 0.7)
        
        self.classifier = nn.Sequential(
            nn.Dropout(final_dropout),
            nn.Linear(fc_input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(final_dropout * 0.5),
            nn.Linear(hidden_size, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
    
    
def objective(trial):
    """Optunaçš„ç›®æ ‡å‡½æ•°"""
    
    # 1. æ¨¡å‹è¶…å‚æ•°
    model = ConfigurableCNN(trial, num_classes=8).to(device)
    
    # 2. è®­ç»ƒè¶…å‚æ•°
    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)
    
    # 3. ä¼˜åŒ–å™¨é€‰æ‹©
    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])
    
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == 'RMSprop':
        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:  # SGD
        momentum = trial.suggest_float('momentum', 0.8, 0.99)
        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)
    
    # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨æ–°çš„batch_sizeï¼‰
    if 'train_dataset' in globals():
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    else:
        # å¦‚æœæ•°æ®é›†ä¸å­˜åœ¨ï¼Œè¿”å›ä¸€ä¸ªè™šæ‹Ÿå€¼ç”¨äºæ¼”ç¤º
        return 0.5
    
    # 5. è®­ç»ƒé…ç½®
    criterion = nn.CrossEntropyLoss()
    n_epochs = 10  # ä¸ºäº†å¿«é€Ÿè°ƒä¼˜ï¼Œå‡å°‘epochæ•°
    
    # 6. è®­ç»ƒå¾ªç¯
    best_val_acc = 0.0
    
    for epoch in range(n_epochs):
        # è®­ç»ƒ
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            if batch_idx > 50:  # é™åˆ¶æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°ï¼ŒåŠ é€Ÿè°ƒä¼˜
                break
                
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
        
        # éªŒè¯
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(val_loader):
                if batch_idx > 20:  # é™åˆ¶éªŒè¯æ‰¹æ¬¡æ•°
                    break
                    
                data, target = data.to(device), target.to(device)
                output = model(data)
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        val_acc = correct / total
        best_val_acc = max(best_val_acc, val_acc)
        
        # æŠ¥å‘Šä¸­é—´ç»“æœç»™Optuna
        trial.report(val_acc, epoch)
        
        # å¦‚æœæ•ˆæœä¸å¥½ï¼Œæå‰å‰ªæ
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()
    
    return best_val_acc

def run_hyperparameter_tuning(n_trials=50):
    """è¿è¡Œè¶…å‚æ•°è°ƒä¼˜"""
    
    print("ğŸ” å¼€å§‹è¶…å‚æ•°è‡ªåŠ¨è°ƒä¼˜...")
    print(f"å°†å°è¯• {n_trials} ç§ä¸åŒçš„è¶…å‚æ•°ç»„åˆ")
    
    # åˆ›å»ºç ”ç©¶å¯¹è±¡
    study = optuna.create_study(
        direction='maximize',  # æœ€å¤§åŒ–éªŒè¯å‡†ç¡®ç‡
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)
    )
    
    # å¼€å§‹ä¼˜åŒ–
    study.optimize(objective, n_trials=n_trials)
    
    # è¾“å‡ºç»“æœ
    print("ğŸ¯ è¶…å‚æ•°è°ƒä¼˜å®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {study.best_value:.4f}")
    print("æœ€ä½³è¶…å‚æ•°ç»„åˆ:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")
    
    return study




def train_baseline():
    print("ğŸ—ï¸  åˆ›å»ºCNNæ¨¡å‹...")
    model = CNN(num_classes=8)
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    model = model.to(device)

    print("\nğŸ“‹ æ¨¡å‹ç»“æ„:")
    print(model)

    criterion = nn.CrossEntropyLoss()

    # ä¼˜åŒ–å™¨ - ä½¿ç”¨ç»å…¸é…ç½®
    optimizer = optim.Adam(
        model.parameters(), 
        lr=0.001,           # æ ‡å‡†å­¦ä¹ ç‡
        weight_decay=1e-4   # L2æ­£åˆ™åŒ–
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨(ä¸è°ƒåº¦)
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 1.0)
    
    batch_size = 32
    num_workers = 0
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,           
        num_workers=num_workers,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False,          
        num_workers=num_workers,
        pin_memory=True
    )
    

    print("å‡†å¤‡åŸºçº¿æ¨¡å‹")

    train_losses, val_losses, train_accs, val_accs = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)
    
    
    

def manual_grid_search():
    """æ‰‹åŠ¨ç½‘æ ¼æœç´¢"""
    
    if 'train_dataset' not in globals():
        print("âš ï¸  è¯·å…ˆè¿è¡Œæ•°æ®åŠ è½½æ­¥éª¤ï¼")
        return None
    
    print("ğŸ” å¼€å§‹æ‰‹åŠ¨ç½‘æ ¼æœç´¢...")
    
    # å®šä¹‰æœç´¢ç©ºé—´
    param_grid = {
        'lr': [0.001, 0.0003, 0.0001],
        'batch_size': [32, 48, 64],              
        'weight_decay': [1e-4, 1e-5, 0],
        'dropout_rate': [0.2, 0.3, 0.5]
    }
    
    best_params = None
    best_acc = 0.0
    results = []
    
    # éå†æ‰€æœ‰å‚æ•°ç»„åˆ
    from itertools import product
    import gc
    
    param_combinations = list(product(*param_grid.values()))
    total_combinations = len(param_combinations)
    
    print(f"æ€»å…±éœ€è¦æµ‹è¯• {total_combinations} ç§å‚æ•°ç»„åˆ")
    
    for i, params in enumerate(param_combinations):
        lr, batch_size, weight_decay, dropout_rate = params
        
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•ç»„åˆ {i+1}/{total_combinations}:")
        print(f"  lr={lr}, batch_size={batch_size}, weight_decay={weight_decay}, dropout={dropout_rate}")

        try:
            # 1. æ¸…ç†ä¹‹å‰çš„è®¡ç®—å›¾å’Œç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            # 2. åˆ›å»ºæ¨¡å‹
            model = CNN(num_classes=8).to(device)
            
            # ä¿®æ”¹dropoutç‡
            for module in model.modules():
                if isinstance(module, nn.Dropout) or isinstance(module, nn.Dropout2d):
                    module.p = dropout_rate
            
            # 3. åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
            criterion = nn.CrossEntropyLoss()
            
            # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader = DataLoader(
                train_dataset, 
                batch_size=batch_size, 
                shuffle=True, 
                num_workers=0,
                pin_memory=True
            )
            val_loader = DataLoader(
                val_dataset, 
                batch_size=batch_size, 
                shuffle=False, 
                num_workers=0,
                pin_memory=True
            )
            
            # 5. å¿«é€Ÿè®­ç»ƒ
            best_val_acc = 0.0
            for epoch in range(8):  # åªè®­ç»ƒ8ä¸ªepoch
                # è®­ç»ƒé˜¶æ®µ
                model.train()
                train_loss = 0.0
                train_batches = 0

                for batch_idx, (data, target) in enumerate(train_loader):
                    data, target = data.to(device), target.to(device)
                    
                    # å‰å‘ä¼ æ’­
                    optimizer.zero_grad()
                    output = model(data)
                    loss = criterion(output, target)
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    optimizer.step()
                    
                    train_loss += loss.item()
                    train_batches += 1
                    
                    # æ¸…ç†ä¸­é—´ç»“æœ
                    del data, target, output, loss
                    
                    # æ¯10ä¸ªbatchæ¸…ç†ä¸€æ¬¡ç¼“å­˜
                    if batch_idx % 10 == 0 and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                
                # éªŒè¯é˜¶æ®µ
                model.eval()
                correct = 0
                total = 0
                val_batches = 0
                
                with torch.no_grad():
                    for batch_idx, (data, target) in enumerate(val_loader):

                        data, target = data.to(device), target.to(device)
                        output = model(data)
                        _, predicted = torch.max(output.data, 1)
                        total += target.size(0)
                        correct += (predicted == target).sum().item()
                        val_batches += 1
                        
                        # æ¸…ç†ä¸­é—´ç»“æœ
                        del data, target, output, predicted
                
                val_acc = correct / total if total > 0 else 0.0
                best_val_acc = max(best_val_acc, val_acc)

                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0.0
                print(f"    Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Acc={val_acc:.4f}")
            
            # 6. ä¿å­˜ç»“æœ
            result = {
                'params': {
                    'lr': lr, 
                    'batch_size': batch_size, 
                    'weight_decay': weight_decay, 
                    'dropout': dropout_rate
                },
                'accuracy': best_val_acc
            }
            results.append(result)
            
            print(f"  âœ“ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
            
            # æ›´æ–°æœ€ä½³å‚æ•°
            if best_val_acc > best_acc:
                best_acc = best_val_acc
                best_params = {
                    'lr': lr, 
                    'batch_size': batch_size, 
                    'weight_decay': weight_decay, 
                    'dropout': dropout_rate
                }
                print(f"  ğŸ¯ å‘ç°æ›´å¥½çš„å‚æ•°ç»„åˆï¼")
            
        except Exception as e:
            print(f"  âŒ å‚æ•°ç»„åˆå¤±è´¥: {str(e)}")
            continue
        
        finally:
            # 7. å¼ºåˆ¶æ¸…ç†èµ„æº
            try:
                # åˆ é™¤æ¨¡å‹å’Œä¼˜åŒ–å™¨
                if 'model' in locals():
                    del model
                if 'optimizer' in locals():
                    del optimizer
                if 'criterion' in locals():
                    del criterion
                if 'train_loader' in locals():
                    del train_loader
                if 'val_loader' in locals():
                    del val_loader
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
                
                # æ¸…ç©ºCUDAç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()  # ç¡®ä¿æ‰€æœ‰CUDAæ“ä½œå®Œæˆ
                
                # æ˜¾ç¤ºå½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
                if torch.cuda.is_available():
                    current_memory = torch.cuda.memory_allocated(device) / 1024**2
                    print(f"  ğŸ’¾ å½“å‰æ˜¾å­˜ä½¿ç”¨: {current_memory:.2f} MB")
                
            except Exception as cleanup_error:
                print(f"  âš ï¸ æ¸…ç†èµ„æºæ—¶å‡ºé”™: {str(cleanup_error)}")
    
    print(f"\nâœ… ç½‘æ ¼æœç´¢å®Œæˆï¼")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
    print("æœ€ä½³å‚æ•°:")
    for key, value in best_params.items():
        print(f"  {key}: {value}")
    
    return best_params, results




if __name__ == "__main__":
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']  
    plt.rcParams['axes.unicode_minus'] = False

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # è®¾ç½®å›¾è¡¨æ ·å¼
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")

    DATA_DIR = "fma_metadata"  
    AUDIO_DIR = "fma_small"   
    FEATURE_PATH = 'features'

    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½ç‰¹å¾æ•°æ®
    features, labels, track_ids = load_all_features()

    if features is not None:
        # åˆ’åˆ†æ•°æ®é›†
        X_train, X_val, X_test, y_train, y_val, y_test = create_data_splits(features, labels)
        
        # åˆ›å»ºæ•°æ®é›†
        train_transform = AudioTransform()  # è®­ç»ƒæ—¶ä½¿ç”¨æ•°æ®å¢å¼º
        
        train_dataset = MusicDataset(X_train, y_train, transform=train_transform)
        val_dataset = MusicDataset(X_val, y_val, transform=None)
        test_dataset = MusicDataset(X_test, y_test, transform=None)
        
        print("âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
        
        

    # train_baseline()   # å·²ç»å®Œæˆï¼Œå·²ä¿å­˜
    
    # run_hyperparameter_tuning(n_trials=20) # éœ€è¦ä¼˜åŒ–ç©ºé—´åˆ†é…ï¼Œæ˜¾å­˜ä¸è¶³

    manual_grid_search()
    