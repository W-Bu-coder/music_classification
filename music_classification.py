import pandas as pd
import numpy as np
import torch
import os
import glob
import librosa
import librosa.display
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch.nn.functional as F
from tqdm import tqdm
import time
import random
import optuna
from optuna.integration import PyTorchLightningPruningCallback
from sklearn.metrics import accuracy_score, classification_report
warnings.filterwarnings('ignore')

def load_all_features():
    """Âä†ËΩΩÊâÄÊúâÁâπÂæÅÊñá‰ª∂ÔºàÂÜÖÂ≠ò‰ºòÂåñÁâàÔºâ"""
    print("üìÇ Âä†ËΩΩÊâÄÊúâÁâπÂæÅÊñá‰ª∂...")

    feature_files = glob.glob(os.path.join(FEATURE_PATH, 'features_*.npz'))
    if not feature_files:
        print("‚ùå Ê≤°ÊúâÊâæÂà∞ÁâπÂæÅÊñá‰ª∂ÔºÅ")
        return None, None, None
    
    # ÂàùÂßãÂåñÁ©∫ÂàóË°®ÔºàÈ¢ÑÂàÜÈÖçÂÜÖÂ≠òÊõ¥È´òÊïàÔºâ
    all_features = []
    all_labels = []
    all_track_ids = []
    
    for file in feature_files:
        print(f"   Âä†ËΩΩ: {file}")
        with np.load(file) as data:  # ‰ΩøÁî®‰∏ä‰∏ãÊñáÁÆ°ÁêÜÂô®Á°Æ‰øùÊñá‰ª∂ÂèäÊó∂ÂÖ≥Èó≠
            # ÂÖ≥ÈîÆ‰øÆÊîπ1ÔºöÁ´ãÂç≥ËΩ¨Êç¢‰∏∫float32ÂáèÂ∞ëÂÜÖÂ≠ò
            all_features.append(data['features'].astype(np.float32))
            all_labels.append(data['labels'])
            all_track_ids.append(data['track_ids'])
    
    # ÂÖ≥ÈîÆ‰øÆÊîπ2ÔºöÂàÜÊ≠•ÂêàÂπ∂+ÂèäÊó∂ÈáäÊîæÂÜÖÂ≠ò
    features = np.concatenate(all_features, axis=0)
    del all_features  # Á´ãÂç≥Âà†Èô§‰∏¥Êó∂ÂèòÈáè
    labels = np.concatenate(all_labels, axis=0)
    track_ids = np.concatenate(all_track_ids, axis=0)
    
    # Âº∫Âà∂ÂûÉÂúæÂõûÊî∂
    import gc
    gc.collect()
    
    print(f"‚úÖ Âä†ËΩΩÂÆåÊàê! ÊÄªÊ†∑Êú¨Êï∞: {len(features)}, ÁâπÂæÅÁª¥Â∫¶: {features.shape}")
    return features, labels, track_ids

class MusicDataset(Dataset):
    def __init__(self, features, labels, transform=None):
        self.features = features
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        feature = self.features[idx]
        label = self.labels[idx]
        
        # ËΩ¨Êç¢‰∏∫Âº†Èáè
        feature = torch.FloatTensor(feature).unsqueeze(0)  # Ê∑ªÂä†ÈÄöÈÅìÁª¥Â∫¶
        label = torch.LongTensor([label])[0]
        
        if self.transform:
            feature = self.transform(feature)
        
        return feature, label

def create_data_splits(features, labels, test_size=0.2, val_size=0.1, random_state=42):
    """
    ÂàõÂª∫ËÆ≠ÁªÉ„ÄÅÈ™åËØÅÂíåÊµãËØïÈõÜ
    """
    print("üîÑ ÂàíÂàÜÊï∞ÊçÆÈõÜ...")
    
    # È¶ñÂÖàÂàÜÁ¶ªÂá∫ÊµãËØïÈõÜ
    X_temp, X_test, y_temp, y_test = train_test_split(
        features, labels, test_size=test_size, 
        random_state=random_state, stratify=labels
    )
    
    # ‰ªéÂâ©‰ΩôÊï∞ÊçÆ‰∏≠ÂàÜÁ¶ªÂá∫È™åËØÅÈõÜ
    val_size_adjusted = val_size / (1 - test_size)  # Ë∞ÉÊï¥È™åËØÅÈõÜÊØî‰æã
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size_adjusted, 
        random_state=random_state, stratify=y_temp
    )
    
    print(f"‚úÖ Êï∞ÊçÆÈõÜÂàíÂàÜÂÆåÊàê:")
    print(f"   ËÆ≠ÁªÉÈõÜ: {len(X_train)} Ê†∑Êú¨ ({len(X_train)/len(features)*100:.1f}%)")
    print(f"   È™åËØÅÈõÜ: {len(X_val)} Ê†∑Êú¨ ({len(X_val)/len(features)*100:.1f}%)")
    print(f"   ÊµãËØïÈõÜ: {len(X_test)} Ê†∑Êú¨ ({len(X_test)/len(features)*100:.1f}%)")
    
    # Ê£ÄÊü•Ê†áÁ≠æÂàÜÂ∏É
    print(f"\nüìä ÂêÑÈõÜÂêàÁöÑÊ†áÁ≠æÂàÜÂ∏É:")
    print(f"   ËÆ≠ÁªÉÈõÜ: {np.bincount(y_train)}")
    print(f"   È™åËØÅÈõÜ: {np.bincount(y_val)}")
    print(f"   ÊµãËØïÈõÜ: {np.bincount(y_test)}")
    
    return X_train, X_val, X_test, y_train, y_val, y_test

# Êï∞ÊçÆÂ¢ûÂº∫
class AudioTransform:
    def __init__(self, noise_factor=0.005, time_shift_factor=0.1):
        self.noise_factor = noise_factor
        self.time_shift_factor = time_shift_factor
    
    def __call__(self, x):
        # Ê∑ªÂä†Âô™Â£∞
        if random.random() > 0.5:
            noise = torch.randn_like(x) * self.noise_factor
            x = x + noise
        
        # Êó∂Èó¥ÂÅèÁßª
        if random.random() > 0.5:
            shift = int(x.shape[-1] * self.time_shift_factor * (random.random() - 0.5))
            if shift != 0:
                if shift > 0:
                    x = torch.cat([x[..., shift:], torch.zeros_like(x[..., :shift])], dim=-1)
                else:
                    x = torch.cat([torch.zeros_like(x[..., :abs(shift)]), x[..., :shift]], dim=-1)
        
        return x


    
class CNN(nn.Module):
    def __init__(self, num_classes=8):
        super(CNN, self).__init__()
        
        self.features = nn.Sequential(
            # Á¨¨‰∏ÄÂ±Ç
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
            
            # Á¨¨‰∫åÂ±Ç
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
            
            # Á¨¨‰∏âÂ±Ç
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
            
            # Á¨¨ÂõõÂ±Ç
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
        )
        
        # Ëá™ÈÄÇÂ∫îÊ±†Âåñ
        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))
        
        # ÂàÜÁ±ªÂô®
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256 * 4 * 4, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x




def train_epoch(model, dataloader, criterion, optimizer, device):
    """ËÆ≠ÁªÉ‰∏Ä‰∏™epoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        with torch.no_grad():  
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

def validate_epoch(model, dataloader, criterion, device):
    """È™åËØÅ‰∏Ä‰∏™epoch"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in dataloader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc

def plot_training_history(train_losses, val_losses, train_accs, val_accs):
    """ÁªòÂà∂ËÆ≠ÁªÉÂéÜÂè≤"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ÊçüÂ§±Êõ≤Á∫ø
    ax1.plot(train_losses, label='Training Loss', color='blue')
    ax1.plot(val_losses, label='Validation Loss', color='red')
    ax1.set_title('Loss vs Epoch')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # ÂáÜÁ°ÆÁéáÊõ≤Á∫ø
    ax2.plot(train_accs, label='Training Accuracy', color='blue')
    ax2.plot(val_accs, label='Validation Accuracy', color='red')
    ax2.set_title('Accuracy vs Epoch')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()


class EarlyStopping:
    """Êó©ÂÅúÊú∫Âà∂"""
    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1

        if self.counter >= self.patience:
            if self.restore_best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False

    def save_checkpoint(self, model):
        self.best_weights = model.state_dict().copy()
        
        
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, 
                num_epochs=50, device='cuda', patience=10):
    """ÂÆåÊï¥ÁöÑËÆ≠ÁªÉÊµÅÁ®ã"""
    
    print(f"üöÄ ÂºÄÂßãËÆ≠ÁªÉÔºåÂÖ± {num_epochs} ‰∏™epoch...")
    
    # ÂàùÂßãÂåñËÆ∞ÂΩï
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    
    # Êó©ÂÅúÊú∫Âà∂
    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)
    
    best_val_acc = 0.0
    start_time = time.time()
    
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        print("-" * 30)
        
        # ËÆ≠ÁªÉ
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        
        # È™åËØÅ
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        
        # Ë∞ÉÊï¥Â≠¶‰π†Áéá
        if scheduler:
            scheduler.step(val_loss)
        
        # ÊâìÂç∞ÁªìÊûú
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        
        # ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_music_model.pth')
            print(f"üéØ Êñ∞ÁöÑÊúÄ‰Ω≥È™åËØÅÂáÜÁ°ÆÁéá: {best_val_acc:.2f}%")
        
        # Êó©ÂÅúÊ£ÄÊü•
        if early_stopping(val_loss, model):
            print(f"‚èπÔ∏è  Êó©ÂÅúËß¶ÂèëÔºåÂú®Á¨¨ {epoch+1} ‰∏™epochÂÅúÊ≠¢ËÆ≠ÁªÉ")
            break
    
    training_time = time.time() - start_time
    print(f"\n‚úÖ ËÆ≠ÁªÉÂÆåÊàêÔºÅ")
    print(f"‚è±Ô∏è  ËÆ≠ÁªÉÊó∂Èó¥: {training_time/60:.2f} ÂàÜÈíü")
    print(f"üèÜ ÊúÄ‰Ω≥È™åËØÅÂáÜÁ°ÆÁéá: {best_val_acc:.2f}%")
    
    return train_losses, val_losses, train_accs, val_accs




class ConfigurableCNN(nn.Module):
    """ÂèØÈÖçÁΩÆÁöÑCNNÊ®°ÂûãÔºåÁî®‰∫éË∂ÖÂèÇÊï∞Ë∞É‰ºò"""
    
    def __init__(self, trial, num_classes=8):
        super(ConfigurableCNN, self).__init__()
        
        # ÈÄöËøátrialÂØπË±°Ëé∑ÂèñË∂ÖÂèÇÊï∞
        self.n_layers = trial.suggest_int('n_layers', 2, 5)
        
        layers = []
        in_channels = 1
        
        for i in range(self.n_layers):
            # ÊØèÂ±ÇÁöÑÈÄöÈÅìÊï∞
            out_channels = trial.suggest_categorical(f'n_units_l{i}', [16, 32, 64, 128, 256])
            
            # Âç∑ÁßØÊ†∏Â§ßÂ∞è
            kernel_size = trial.suggest_categorical(f'kernel_size_l{i}', [3, 5])
            
            # DropoutÊ¶ÇÁéá
            dropout_rate = trial.suggest_float(f'dropout_l{i}', 0.1, 0.5)
            
            # ÊûÑÂª∫Âç∑ÁßØÂùó
            layers.extend([
                nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(),
                nn.MaxPool2d(2, 2),
                nn.Dropout2d(dropout_rate)
            ])
            
            in_channels = out_channels
        
        self.features = nn.Sequential(*layers)
        
        # Ëá™ÈÄÇÂ∫îÊ±†Âåñ
        pool_size = trial.suggest_categorical('pool_size', [2, 4, 8])
        self.adaptive_pool = nn.AdaptiveAvgPool2d((pool_size, pool_size))
        
        # ÂàÜÁ±ªÂô®
        fc_input_size = in_channels * pool_size * pool_size
        hidden_size = trial.suggest_categorical('fc_hidden_size', [128, 256, 512])
        final_dropout = trial.suggest_float('final_dropout', 0.2, 0.7)
        
        self.classifier = nn.Sequential(
            nn.Dropout(final_dropout),
            nn.Linear(fc_input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(final_dropout * 0.5),
            nn.Linear(hidden_size, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
    
    
def objective(trial):
    """OptunaÁöÑÁõÆÊ†áÂáΩÊï∞"""
    
    # 1. Ê®°ÂûãË∂ÖÂèÇÊï∞
    model = ConfigurableCNN(trial, num_classes=8).to(device)
    
    # 2. ËÆ≠ÁªÉË∂ÖÂèÇÊï∞
    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)
    
    # 3. ‰ºòÂåñÂô®ÈÄâÊã©
    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])
    
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == 'RMSprop':
        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:  # SGD
        momentum = trial.suggest_float('momentum', 0.8, 0.99)
        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)
    
    # 4. ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®Ôºà‰ΩøÁî®Êñ∞ÁöÑbatch_sizeÔºâ
    if 'train_dataset' in globals():
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    else:
        # Â¶ÇÊûúÊï∞ÊçÆÈõÜ‰∏çÂ≠òÂú®ÔºåËøîÂõû‰∏Ä‰∏™ËôöÊãüÂÄºÁî®‰∫éÊºîÁ§∫
        return 0.5
    
    # 5. ËÆ≠ÁªÉÈÖçÁΩÆ
    criterion = nn.CrossEntropyLoss()
    n_epochs = 10  # ‰∏∫‰∫ÜÂø´ÈÄüË∞É‰ºòÔºåÂáèÂ∞ëepochÊï∞
    
    # 6. ËÆ≠ÁªÉÂæ™ÁéØ
    best_val_acc = 0.0
    
    for epoch in range(n_epochs):
        # ËÆ≠ÁªÉ
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            if batch_idx > 50:  # ÈôêÂà∂ÊØè‰∏™epochÁöÑÊâπÊ¨°Êï∞ÔºåÂä†ÈÄüË∞É‰ºò
                break
                
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
        
        # È™åËØÅ
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(val_loader):
                if batch_idx > 20:  # ÈôêÂà∂È™åËØÅÊâπÊ¨°Êï∞
                    break
                    
                data, target = data.to(device), target.to(device)
                output = model(data)
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        val_acc = correct / total
        best_val_acc = max(best_val_acc, val_acc)
        
        # Êä•Âëä‰∏≠Èó¥ÁªìÊûúÁªôOptuna
        trial.report(val_acc, epoch)
        
        # Â¶ÇÊûúÊïàÊûú‰∏çÂ•ΩÔºåÊèêÂâçÂâ™Êûù
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()
    
    return best_val_acc

def run_hyperparameter_tuning(n_trials=50):
    """ËøêË°åË∂ÖÂèÇÊï∞Ë∞É‰ºò"""
    
    print("üîç ÂºÄÂßãË∂ÖÂèÇÊï∞Ëá™Âä®Ë∞É‰ºò...")
    print(f"Â∞ÜÂ∞ùËØï {n_trials} Áßç‰∏çÂêåÁöÑË∂ÖÂèÇÊï∞ÁªÑÂêà")
    
    # ÂàõÂª∫Á†îÁ©∂ÂØπË±°
    study = optuna.create_study(
        direction='maximize',  # ÊúÄÂ§ßÂåñÈ™åËØÅÂáÜÁ°ÆÁéá
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)
    )
    
    # ÂºÄÂßã‰ºòÂåñ
    study.optimize(objective, n_trials=n_trials)
    
    # ËæìÂá∫ÁªìÊûú
    print("üéØ Ë∂ÖÂèÇÊï∞Ë∞É‰ºòÂÆåÊàêÔºÅ")
    print(f"ÊúÄ‰Ω≥È™åËØÅÂáÜÁ°ÆÁéá: {study.best_value:.4f}")
    print("ÊúÄ‰Ω≥Ë∂ÖÂèÇÊï∞ÁªÑÂêà:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")
    
    return study




def train_baseline():
    print("üèóÔ∏è  ÂàõÂª∫CNNÊ®°Âûã...")
    model = CNN(num_classes=8)
    print(f"Ê®°ÂûãÂèÇÊï∞Èáè: {sum(p.numel() for p in model.parameters()):,}")
    model = model.to(device)

    print("\nüìã Ê®°ÂûãÁªìÊûÑ:")
    print(model)

    criterion = nn.CrossEntropyLoss()

    # ‰ºòÂåñÂô® - ‰ΩøÁî®ÁªèÂÖ∏ÈÖçÁΩÆ
    optimizer = optim.Adam(
        model.parameters(), 
        lr=0.001,           # Ê†áÂáÜÂ≠¶‰π†Áéá
        weight_decay=1e-4   # L2Ê≠£ÂàôÂåñ
    )

    # Â≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®(‰∏çË∞ÉÂ∫¶)
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 1.0)
    
    batch_size = 32
    num_workers = 0
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,           
        num_workers=num_workers,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False,          
        num_workers=num_workers,
        pin_memory=True
    )
    

    print("ÂáÜÂ§áÂü∫Á∫øÊ®°Âûã")

    train_losses, val_losses, train_accs, val_accs = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)
    
    
    

def manual_grid_search():
    """ÊâãÂä®ÁΩëÊ†ºÊêúÁ¥¢"""
    
    if 'train_dataset' not in globals():
        print("‚ö†Ô∏è  ËØ∑ÂÖàËøêË°åÊï∞ÊçÆÂä†ËΩΩÊ≠•È™§ÔºÅ")
        return None
    
    print("üîç ÂºÄÂßãÊâãÂä®ÁΩëÊ†ºÊêúÁ¥¢...")
    
    # ÂÆö‰πâÊêúÁ¥¢Á©∫Èó¥
    param_grid = {
        'lr': [0.001, 0.0003, 0.0001],
        'batch_size': [32, 48, 64],              
        'weight_decay': [1e-4, 1e-5, 0],
        'dropout_rate': [0.2, 0.3, 0.5]
    }
    
    best_params = None
    best_acc = 0.0
    results = []
    
    # ÈÅçÂéÜÊâÄÊúâÂèÇÊï∞ÁªÑÂêà
    from itertools import product
    import gc
    
    param_combinations = list(product(*param_grid.values()))
    total_combinations = len(param_combinations)
    
    print(f"ÊÄªÂÖ±ÈúÄË¶ÅÊµãËØï {total_combinations} ÁßçÂèÇÊï∞ÁªÑÂêà")
    
    for i, params in enumerate(param_combinations):
        lr, batch_size, weight_decay, dropout_rate = params
        
        print(f"\n{'='*50}")
        print(f"ÊµãËØïÁªÑÂêà {i+1}/{total_combinations}:")
        print(f"  lr={lr}, batch_size={batch_size}, weight_decay={weight_decay}, dropout={dropout_rate}")

        try:
            # 1. Ê∏ÖÁêÜ‰πãÂâçÁöÑËÆ°ÁÆóÂõæÂíåÁºìÂ≠ò
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            # 2. ÂàõÂª∫Ê®°Âûã
            model = CNN(num_classes=8).to(device)
            
            # ‰øÆÊîπdropoutÁéá
            for module in model.modules():
                if isinstance(module, nn.Dropout) or isinstance(module, nn.Dropout2d):
                    module.p = dropout_rate
            
            # 3. ÂàõÂª∫‰ºòÂåñÂô®ÂíåÊçüÂ§±ÂáΩÊï∞
            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
            criterion = nn.CrossEntropyLoss()
            
            # 4. ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®
            train_loader = DataLoader(
                train_dataset, 
                batch_size=batch_size, 
                shuffle=True, 
                num_workers=0,
                pin_memory=True
            )
            val_loader = DataLoader(
                val_dataset, 
                batch_size=batch_size, 
                shuffle=False, 
                num_workers=0,
                pin_memory=True
            )
            
            # 5. Âø´ÈÄüËÆ≠ÁªÉ
            best_val_acc = 0.0
            for epoch in range(8):  # Âè™ËÆ≠ÁªÉ8‰∏™epoch
                # ËÆ≠ÁªÉÈò∂ÊÆµ
                model.train()
                train_loss = 0.0
                train_batches = 0

                for batch_idx, (data, target) in enumerate(train_loader):
                    data, target = data.to(device), target.to(device)
                    
                    # ÂâçÂêë‰º†Êí≠
                    optimizer.zero_grad()
                    output = model(data)
                    loss = criterion(output, target)
                    
                    # ÂèçÂêë‰º†Êí≠
                    loss.backward()
                    optimizer.step()
                    
                    train_loss += loss.item()
                    train_batches += 1
                    
                    # Ê∏ÖÁêÜ‰∏≠Èó¥ÁªìÊûú
                    del data, target, output, loss
                    
                    # ÊØè10‰∏™batchÊ∏ÖÁêÜ‰∏ÄÊ¨°ÁºìÂ≠ò
                    if batch_idx % 10 == 0 and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                
                # È™åËØÅÈò∂ÊÆµ
                model.eval()
                correct = 0
                total = 0
                val_batches = 0
                
                with torch.no_grad():
                    for batch_idx, (data, target) in enumerate(val_loader):

                        data, target = data.to(device), target.to(device)
                        output = model(data)
                        _, predicted = torch.max(output.data, 1)
                        total += target.size(0)
                        correct += (predicted == target).sum().item()
                        val_batches += 1
                        
                        # Ê∏ÖÁêÜ‰∏≠Èó¥ÁªìÊûú
                        del data, target, output, predicted
                
                val_acc = correct / total if total > 0 else 0.0
                best_val_acc = max(best_val_acc, val_acc)

                avg_train_loss = train_loss / train_batches if train_batches > 0 else 0.0
                print(f"    Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Acc={val_acc:.4f}")
            
            # 6. ‰øùÂ≠òÁªìÊûú
            result = {
                'params': {
                    'lr': lr, 
                    'batch_size': batch_size, 
                    'weight_decay': weight_decay, 
                    'dropout': dropout_rate
                },
                'accuracy': best_val_acc
            }
            results.append(result)
            
            print(f"  ‚úì ÊúÄ‰Ω≥È™åËØÅÂáÜÁ°ÆÁéá: {best_val_acc:.4f}")
            
            # Êõ¥Êñ∞ÊúÄ‰Ω≥ÂèÇÊï∞
            if best_val_acc > best_acc:
                best_acc = best_val_acc
                best_params = {
                    'lr': lr, 
                    'batch_size': batch_size, 
                    'weight_decay': weight_decay, 
                    'dropout': dropout_rate
                }
                print(f"  üéØ ÂèëÁé∞Êõ¥Â•ΩÁöÑÂèÇÊï∞ÁªÑÂêàÔºÅ")
            
        except Exception as e:
            print(f"  ‚ùå ÂèÇÊï∞ÁªÑÂêàÂ§±Ë¥•: {str(e)}")
            continue
        
        finally:
            # 7. Âº∫Âà∂Ê∏ÖÁêÜËµÑÊ∫ê
            try:
                # Âà†Èô§Ê®°ÂûãÂíå‰ºòÂåñÂô®
                if 'model' in locals():
                    del model
                if 'optimizer' in locals():
                    del optimizer
                if 'criterion' in locals():
                    del criterion
                if 'train_loader' in locals():
                    del train_loader
                if 'val_loader' in locals():
                    del val_loader
                
                # Âº∫Âà∂ÂûÉÂúæÂõûÊî∂
                gc.collect()
                
                # Ê∏ÖÁ©∫CUDAÁºìÂ≠ò
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()  # Á°Æ‰øùÊâÄÊúâCUDAÊìç‰ΩúÂÆåÊàê
                
                # ÊòæÁ§∫ÂΩìÂâçÊòæÂ≠ò‰ΩøÁî®ÊÉÖÂÜµ
                if torch.cuda.is_available():
                    current_memory = torch.cuda.memory_allocated(device) / 1024**2
                    print(f"  üíæ ÂΩìÂâçÊòæÂ≠ò‰ΩøÁî®: {current_memory:.2f} MB")
                
            except Exception as cleanup_error:
                print(f"  ‚ö†Ô∏è Ê∏ÖÁêÜËµÑÊ∫êÊó∂Âá∫Èîô: {str(cleanup_error)}")
    
    print(f"\n‚úÖ ÁΩëÊ†ºÊêúÁ¥¢ÂÆåÊàêÔºÅ")
    print(f"ÊúÄ‰Ω≥ÂáÜÁ°ÆÁéá: {best_acc:.4f}")
    print("ÊúÄ‰Ω≥ÂèÇÊï∞:")
    for key, value in best_params.items():
        print(f"  {key}: {value}")
    
    return best_params, results




if __name__ == "__main__":
    
    # ËÆæÁΩÆ‰∏≠ÊñáÂ≠ó‰ΩìÊòæÁ§∫
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']  
    plt.rcParams['axes.unicode_minus'] = False

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ËÆæÁΩÆÂõæË°®Ê†∑Âºè
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")

    DATA_DIR = "fma_metadata"  
    AUDIO_DIR = "fma_small"   
    FEATURE_PATH = 'features'

    print(f"‰ΩøÁî®ËÆæÂ§á: {device}")
    
    # Âä†ËΩΩÁâπÂæÅÊï∞ÊçÆ
    features, labels, track_ids = load_all_features()

    if features is not None:
        # ÂàíÂàÜÊï∞ÊçÆÈõÜ
        X_train, X_val, X_test, y_train, y_val, y_test = create_data_splits(features, labels)
        
        # ÂàõÂª∫Êï∞ÊçÆÈõÜ
        train_transform = AudioTransform()  # ËÆ≠ÁªÉÊó∂‰ΩøÁî®Êï∞ÊçÆÂ¢ûÂº∫
        
        train_dataset = MusicDataset(X_train, y_train, transform=train_transform)
        val_dataset = MusicDataset(X_val, y_val, transform=None)
        test_dataset = MusicDataset(X_test, y_test, transform=None)
        
        print("‚úÖ Êï∞ÊçÆÈõÜÂàõÂª∫ÂÆåÊàêÔºÅ")
        
        

    # train_baseline()   # Â∑≤ÁªèÂÆåÊàêÔºåÂ∑≤‰øùÂ≠ò
    
    # run_hyperparameter_tuning(n_trials=20) # ÈúÄË¶Å‰ºòÂåñÁ©∫Èó¥ÂàÜÈÖçÔºåÊòæÂ≠ò‰∏çË∂≥

    manual_grid_search()
    