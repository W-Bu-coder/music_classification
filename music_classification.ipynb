{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3012995c-a2f5-455c-ae05-a730ec6ee281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置中文字体显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']  \n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 设置图表样式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "DATA_DIR = \"fma_metadata\"  \n",
    "AUDIO_DIR = \"fma_small\"   \n",
    "FEATURE_PATH = 'features'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150b38fe-743f-4100-b48b-a33d62e22b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据: 106574 首歌曲\n",
      "清理后数据: 49598 首歌曲\n"
     ]
    }
   ],
   "source": [
    "def load_metadata():\n",
    "    tracks = pd.read_csv(os.path.join(DATA_DIR, \"tracks.csv\"), header=[0, 1], index_col=0)\n",
    "    genres = pd.read_csv(os.path.join(DATA_DIR, \"genres.csv\"), index_col=0)\n",
    "    features = pd.read_csv(os.path.join(DATA_DIR, \"features.csv\"), header=[0, 1, 2], index_col=0)\n",
    "    return tracks, genres, features\n",
    "\n",
    "def process_tracks_data(tracks):\n",
    "    # 提取关键列\n",
    "    track_data = pd.DataFrame({\n",
    "        'track_id': tracks.index,\n",
    "        'title': tracks[('track', 'title')],\n",
    "        'duration': tracks[('track', 'duration')],\n",
    "        'genre_top': tracks[('track', 'genre_top')],\n",
    "        'genres': tracks[('track', 'genres')],\n",
    "        'listens': tracks[('track', 'listens')],\n",
    "        'bit_rate': tracks[('track', 'bit_rate')],\n",
    "        'interest': tracks[('track', 'interest')]\n",
    "    })\n",
    "    \n",
    "    print(f\"原始数据: {len(track_data)} 首歌曲\")\n",
    "    \n",
    "    # 清理数据\n",
    "    # 移除没有genre_top标签的数据\n",
    "    track_data = track_data.dropna(subset=['genre_top'])\n",
    "    track_data = track_data[track_data['genre_top'] != 0]\n",
    "    \n",
    "    # 转换数据类型\n",
    "    track_data['duration'] = pd.to_numeric(track_data['duration'], errors='coerce')\n",
    "    track_data['listens'] = pd.to_numeric(track_data['listens'], errors='coerce')\n",
    "    genre_name_to_id = {}\n",
    "    for gid, row in genres.iterrows():\n",
    "        genre_name_to_id[row['title']] = gid\n",
    "    \n",
    "    # 映射流派名称到ID\n",
    "    def map_genre_name_to_id(genre_name):\n",
    "        if pd.isna(genre_name):\n",
    "            return np.nan\n",
    "        if isinstance(genre_name, str):\n",
    "            return genre_name_to_id.get(genre_name, np.nan)\n",
    "        else:\n",
    "            return genre_name\n",
    "    \n",
    "    track_data['genre_top'] = track_data['genre_top'].apply(map_genre_name_to_id)\n",
    "    \n",
    "    # 移除无法映射的流派\n",
    "    track_data = track_data.dropna(subset=['genre_top'])\n",
    "    \n",
    "    print(f\"清理后数据: {len(track_data)} 首歌曲\")\n",
    "    return track_data\n",
    "\n",
    "tracks, genres, features = load_metadata()\n",
    "track_data = process_tracks_data(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7707683e-f37f-4616-8732-2b3be634ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 扫描音频目录: fma_small\n",
      "   找到 7999 个音频文件\n",
      "   总大小: 7.43 GB\n"
     ]
    }
   ],
   "source": [
    "def scan_audio_files(audio_dir):\n",
    "    \"\"\"扫描音频目录，找到所有可用的mp3文件\"\"\"\n",
    "    print(f\"📂 扫描音频目录: {audio_dir}\")\n",
    "    \n",
    "    if not os.path.exists(audio_dir):\n",
    "        print(f\"❌ 音频目录不存在: {audio_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    audio_files = {}\n",
    "    total_size = 0\n",
    "    \n",
    "    # 遍历所有子文件夹\n",
    "    for root, dirs, files in os.walk(audio_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp3'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # 从文件名提取track_id\n",
    "                try:\n",
    "                    track_id = int(file.split('.')[0])\n",
    "                    file_size = os.path.getsize(file_path)\n",
    "                    \n",
    "                    audio_files[track_id] = {\n",
    "                        'path': file_path,\n",
    "                        'size_bytes': file_size,\n",
    "                        'size_kb': file_size / 1024,\n",
    "                        'size_mb': file_size / (1024 * 1024)\n",
    "                    }\n",
    "                    total_size += file_size\n",
    "                    \n",
    "                except ValueError:\n",
    "                    # 文件名不是数字，跳过\n",
    "                    continue\n",
    "    \n",
    "    print(f\"   找到 {len(audio_files)} 个音频文件\")\n",
    "    print(f\"   总大小: {total_size / (1024**3):.2f} GB\")\n",
    "    \n",
    "    return audio_files\n",
    "\n",
    "# 扫描音频文件\n",
    "audio_files_info = scan_audio_files(AUDIO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb1cf56-0eca-4f70-aa97-94eb9c8c8908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 过滤结果:\n",
      "   原始track_data: 49598\n",
      "   最终可用数据: 7996\n"
     ]
    }
   ],
   "source": [
    "def filter_valid_audio_files(audio_files_info, track_data, min_size_kb=10, max_size_kb=20000):\n",
    "    \"\"\"过滤出可用的音频文件\"\"\"\n",
    "    valid_files = {}\n",
    "    filtered_tracks = []\n",
    "    \n",
    "    for track_id, file_info in audio_files_info.items():\n",
    "        # 检查文件大小\n",
    "        if not (min_size_kb <= file_info['size_kb'] <= max_size_kb):\n",
    "            continue\n",
    "            \n",
    "        # 检查是否在track_data中存在\n",
    "        if track_id not in track_data['track_id'].values:\n",
    "            continue\n",
    "            \n",
    "        # 获取对应的track信息\n",
    "        track_row = track_data[track_data['track_id'] == track_id].iloc[0]\n",
    "        \n",
    "        # 合并音频文件信息和元数据\n",
    "        combined_info = {\n",
    "            'track_id': track_id,\n",
    "            'title': track_row['title'],\n",
    "            'genre_top': track_row['genre_top'],\n",
    "            'duration': track_row['duration'],\n",
    "            'listens': track_row['listens'],\n",
    "            'audio_path': file_info['path'],\n",
    "            'file_size_kb': file_info['size_kb'],\n",
    "            'file_size_mb': file_info['size_mb']\n",
    "        }\n",
    "        \n",
    "        valid_files[track_id] = file_info\n",
    "        filtered_tracks.append(combined_info)\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    valid_tracks_df = pd.DataFrame(filtered_tracks)\n",
    "    \n",
    "    print(f\"\\n✅ 过滤结果:\")\n",
    "    print(f\"   原始track_data: {len(track_data)}\")\n",
    "    print(f\"   最终可用数据: {len(valid_tracks_df)}\")\n",
    "    \n",
    "    return valid_tracks_df, valid_files\n",
    "\n",
    "# 过滤可用文件\n",
    "valid_tracks_df, valid_audio_files = filter_valid_audio_files(\n",
    "    audio_files_info, track_data, min_size_kb=10, max_size_kb=20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1616e5f-a548-47c8-8a24-4f98bcfe9755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   总流派数: 8\n",
      "   总歌曲数: 7996\n",
      "    1. Hip-Hop        : 1000 ( 12.5%)\n",
      "    2. Pop            : 1000 ( 12.5%)\n",
      "    3. Folk           : 1000 ( 12.5%)\n",
      "    4. International  : 1000 ( 12.5%)\n",
      "    5. Instrumental   : 1000 ( 12.5%)\n",
      "    6. Experimental   :  999 ( 12.5%)\n",
      "    7. Rock           :  999 ( 12.5%)\n",
      "    8. Electronic     :  998 ( 12.5%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_genre_distribution(valid_tracks_df, genres):\n",
    "    \"\"\"分析可用数据的流派分布\"\"\"\n",
    "\n",
    "    # 统计每个流派的歌曲数量\n",
    "    genre_counts = valid_tracks_df['genre_top'].value_counts()\n",
    "    \n",
    "    # 获取流派名称\n",
    "    genre_names = {}\n",
    "    for genre_id in genre_counts.index:\n",
    "        if genre_id in genres.index:\n",
    "            genre_names[genre_id] = genres.loc[genre_id, 'title']\n",
    "        else:\n",
    "            genre_names[genre_id] = f\"Unknown_{genre_id}\"\n",
    "    \n",
    "    # 创建流派统计\n",
    "    genre_stats_filtered = pd.DataFrame({\n",
    "        'genre_id': genre_counts.index,\n",
    "        'genre_name': [genre_names[gid] for gid in genre_counts.index],\n",
    "        'track_count': genre_counts.values,\n",
    "        'percentage': (genre_counts.values / len(valid_tracks_df) * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    print(f\"   总流派数: {len(genre_stats_filtered)}\")\n",
    "    print(f\"   总歌曲数: {len(valid_tracks_df)}\")\n",
    "    for i, row in genre_stats_filtered.head(10).iterrows():\n",
    "        print(f\"   {i+1:2d}. {row['genre_name']:15s}: {row['track_count']:4d} ({row['percentage']:5.1f}%)\")\n",
    "    \n",
    "    return genre_stats_filtered\n",
    "\n",
    "# 分析可用数据的流派分布\n",
    "if len(valid_tracks_df) > 0:\n",
    "    final_genre_stats = analyze_genre_distribution(valid_tracks_df, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a536b91-539c-4f8c-9491-46c35166938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(audio_path, sr=22050, n_mels=128, hop_length=512, n_fft=2048, duration=30):\n",
    "    \"\"\"\n",
    "    提取mel频谱图特征\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 加载音频，限制时长为30秒\n",
    "        y, sr = librosa.load(audio_path, sr=sr, duration=duration, offset=0)\n",
    "        \n",
    "        # 如果音频太短，用零填充到30秒\n",
    "        target_length = sr * duration\n",
    "        if len(y) < target_length:\n",
    "            y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "        else:\n",
    "            y = y[:target_length]\n",
    "        \n",
    "        # 提取mel频谱图\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_mels=n_mels, hop_length=hop_length, n_fft=n_fft\n",
    "        )\n",
    "        \n",
    "        # 转换为对数刻度\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        return mel_spec_db\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理音频文件时出错 {audio_path}: {e}\")\n",
    "        # 返回零填充的频谱图\n",
    "        return np.zeros((n_mels, 1292))  # 30秒音频的默认时间帧数\n",
    "\n",
    "def batch_extract_features(valid_tracks_df, batch_size=100, save_interval=500):\n",
    "    \"\"\"\n",
    "    批量提取特征并保存到磁盘\n",
    "    \"\"\"\n",
    "    print(\"🎵 开始批量提取mel频谱图特征...\")\n",
    " \n",
    "    os.makedirs(FEATURE_PATH, exist_ok=True)\n",
    "    \n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    track_ids_list = []\n",
    "    \n",
    "    # 创建标签编码器\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # 获取所有流派ID并编码\n",
    "    all_genres = valid_tracks_df['genre_top'].values\n",
    "    encoded_labels = label_encoder.fit_transform(all_genres)\n",
    "    \n",
    "    print(f\"流派映射关系:\")\n",
    "    for i, genre_id in enumerate(label_encoder.classes_):\n",
    "        genre_name = genres.loc[genre_id, 'title'] if genre_id in genres.index else f\"Unknown_{genre_id}\"\n",
    "        print(f\"   {i}: {genre_name} (ID: {genre_id})\")\n",
    "    \n",
    "    # 随机打乱数据\n",
    "    indices = np.random.permutation(len(valid_tracks_df))\n",
    "    \n",
    "    processed_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for idx in tqdm(indices, desc=\"提取特征\"):\n",
    "        row = valid_tracks_df.iloc[idx]\n",
    "        \n",
    "        # 提取mel频谱图\n",
    "        mel_spec = extract_mel_spectrogram(row['audio_path'])\n",
    "        \n",
    "        if mel_spec is not None:\n",
    "            features_list.append(mel_spec)\n",
    "            labels_list.append(encoded_labels[idx])\n",
    "            track_ids_list.append(row['track_id'])\n",
    "            processed_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "        \n",
    "        # 定期保存和清理内存\n",
    "        if len(features_list) >= save_interval:\n",
    "            print(f\"\\n💾 保存中间结果... (已处理: {processed_count}, 失败: {failed_count})\")\n",
    "            \n",
    "            # 转换为numpy数组并保存\n",
    "            features_array = np.array(features_list)\n",
    "            labels_array = np.array(labels_list)\n",
    "            track_ids_array = np.array(track_ids_list)\n",
    "            \n",
    "            # 保存到文件\n",
    "            timestamp = int(time.time())\n",
    "            np.savez_compressed(os.path.join(FEATURE_PATH, f'features_batch_{timestamp}.npz'), \n",
    "                              features=features_array, \n",
    "                              labels=labels_array,\n",
    "                              track_ids=track_ids_array)\n",
    "            \n",
    "            # 清理内存\n",
    "            features_list = []\n",
    "            labels_list = []\n",
    "            track_ids_list = []\n",
    "    \n",
    "    # 保存剩余的数据\n",
    "    if features_list:\n",
    "        print(f\"\\n💾 保存最后批次...\")\n",
    "        features_array = np.array(features_list)\n",
    "        labels_array = np.array(labels_list)\n",
    "        track_ids_array = np.array(track_ids_list)\n",
    "        \n",
    "        timestamp = int(time.time())\n",
    "        np.savez_compressed(os.path.join(FEATURE_PATH, f'features_batch_{timestamp}.npz'), \n",
    "                          features=features_array, \n",
    "                          labels=labels_array,\n",
    "                          track_ids=track_ids_array)\n",
    "    \n",
    "    print(f\"\\n特征提取完成!\")\n",
    "    print(f\"   成功处理: {processed_count} 个文件\")\n",
    "    print(f\"   失败: {failed_count} 个文件\")\n",
    "    \n",
    "    return label_encoder\n",
    "\n",
    "# test_data = valid_tracks_df.head(100)  # 只处理前100个文件进行测试\n",
    "# label_encoder = batch_extract_features(test_data, batch_size=50, save_interval=50)\n",
    "\n",
    "# 已经提取过\n",
    "# label_encoder = batch_extract_features(valid_tracks_df, batch_size=100, save_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7249ad17-3087-4d8a-b4d1-e32c905a9763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_features():\n",
    "    \"\"\"\n",
    "    加载所有保存的特征文件\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"📂 加载所有特征文件...\")\n",
    "\n",
    "    feature_files = glob.glob(os.path.join(FEATURE_PATH, 'features_*.npz'))\n",
    "    if not feature_files:\n",
    "        print(\"❌ 没有找到特征文件！\")\n",
    "        return None, None, None\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_track_ids = []\n",
    "    \n",
    "    for file in feature_files:\n",
    "        print(f\"   加载: {file}\")\n",
    "        data = np.load(file)\n",
    "        all_features.append(data['features'])\n",
    "        all_labels.append(data['labels'])\n",
    "        all_track_ids.append(data['track_ids'])\n",
    "    \n",
    "    # 合并所有数据\n",
    "    features = np.concatenate(all_features, axis=0)\n",
    "    labels = np.concatenate(all_labels, axis=0)\n",
    "    track_ids = np.concatenate(all_track_ids, axis=0)\n",
    "    \n",
    "    print(f\"✅ 加载完成!\")\n",
    "    print(f\"   总样本数: {len(features)}\")\n",
    "    print(f\"   特征维度: {features.shape}\")\n",
    "    print(f\"   标签分布: {np.bincount(labels)}\")\n",
    "    \n",
    "    return features, labels, track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f3ad512-6808-4a88-b865-995506f58c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 转换为张量\n",
    "        feature = torch.FloatTensor(feature).unsqueeze(0)  # 添加通道维度\n",
    "        label = torch.LongTensor([label])[0]\n",
    "        \n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "        \n",
    "        return feature, label\n",
    "\n",
    "def create_data_splits(features, labels, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    创建训练、验证和测试集\n",
    "    \"\"\"\n",
    "    print(\"🔄 划分数据集...\")\n",
    "    \n",
    "    # 首先分离出测试集\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        features, labels, test_size=test_size, \n",
    "        random_state=random_state, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # 从剩余数据中分离出验证集\n",
    "    val_size_adjusted = val_size / (1 - test_size)  # 调整验证集比例\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted, \n",
    "        random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 数据集划分完成:\")\n",
    "    print(f\"   训练集: {len(X_train)} 样本 ({len(X_train)/len(features)*100:.1f}%)\")\n",
    "    print(f\"   验证集: {len(X_val)} 样本 ({len(X_val)/len(features)*100:.1f}%)\")\n",
    "    print(f\"   测试集: {len(X_test)} 样本 ({len(X_test)/len(features)*100:.1f}%)\")\n",
    "    \n",
    "    # 检查标签分布\n",
    "    print(f\"\\n📊 各集合的标签分布:\")\n",
    "    print(f\"   训练集: {np.bincount(y_train)}\")\n",
    "    print(f\"   验证集: {np.bincount(y_val)}\")\n",
    "    print(f\"   测试集: {np.bincount(y_test)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "871b7794-734e-427f-aa3f-fb50e9b2ad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 加载所有特征文件...\n",
      "   加载: features\\features_batch_1754283560.npz\n",
      "   加载: features\\features_batch_1754283609.npz\n",
      "   加载: features\\features_batch_1754283656.npz\n",
      "   加载: features\\features_batch_1754283721.npz\n",
      "   加载: features\\features_batch_1754283766.npz\n",
      "   加载: features\\features_batch_1754283828.npz\n",
      "   加载: features\\features_batch_1754283986.npz\n",
      "   加载: features\\features_batch_1754284160.npz\n",
      "   加载: features\\features_batch_1754284344.npz\n",
      "   加载: features\\features_batch_1754284508.npz\n",
      "   加载: features\\features_batch_1754284602.npz\n",
      "   加载: features\\features_batch_1754284667.npz\n",
      "   加载: features\\features_batch_1754284712.npz\n",
      "   加载: features\\features_batch_1754284758.npz\n",
      "   加载: features\\features_batch_1754284804.npz\n",
      "   加载: features\\features_batch_1754284850.npz\n",
      "   加载: features\\features_batch_1754285711.npz\n",
      "✅ 加载完成!\n",
      "   总样本数: 8496\n",
      "   特征维度: (8496, 128, 1292)\n",
      "   标签分布: [1065 1064 1066 1059 1061 1059 1062 1060]\n",
      "🔄 划分数据集...\n",
      "✅ 数据集划分完成:\n",
      "   训练集: 5946 样本 (70.0%)\n",
      "   验证集: 850 样本 (10.0%)\n",
      "   测试集: 1700 样本 (20.0%)\n",
      "\n",
      "📊 各集合的标签分布:\n",
      "   训练集: [745 745 746 741 743 741 743 742]\n",
      "   验证集: [107 106 107 106 106 106 106 106]\n",
      "   测试集: [213 213 213 212 212 212 213 212]\n",
      "✅ 数据集创建完成！\n"
     ]
    }
   ],
   "source": [
    "# 数据增强\n",
    "class AudioTransform:\n",
    "    def __init__(self, noise_factor=0.005, time_shift_factor=0.1):\n",
    "        self.noise_factor = noise_factor\n",
    "        self.time_shift_factor = time_shift_factor\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # 添加噪声\n",
    "        if random.random() > 0.5:\n",
    "            noise = torch.randn_like(x) * self.noise_factor\n",
    "            x = x + noise\n",
    "        \n",
    "        # 时间偏移\n",
    "        if random.random() > 0.5:\n",
    "            shift = int(x.shape[-1] * self.time_shift_factor * (random.random() - 0.5))\n",
    "            if shift != 0:\n",
    "                if shift > 0:\n",
    "                    x = torch.cat([x[..., shift:], torch.zeros_like(x[..., :shift])], dim=-1)\n",
    "                else:\n",
    "                    x = torch.cat([torch.zeros_like(x[..., :abs(shift)]), x[..., :shift]], dim=-1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 加载特征数据\n",
    "features, labels, track_ids = load_all_features()\n",
    "\n",
    "if features is not None:\n",
    "    # 划分数据集\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = create_data_splits(features, labels)\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_transform = AudioTransform()  # 训练时使用数据增强\n",
    "    \n",
    "    train_dataset = MusicDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = MusicDataset(X_val, y_val, transform=None)\n",
    "    test_dataset = MusicDataset(X_test, y_test, transform=None)\n",
    "    \n",
    "    print(\"✅ 数据集创建完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed849e4b-3ada-4c47-a6b8-d1d3c5498115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # 第一层\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # 第二层\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # 第三层\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # 第四层\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "        )\n",
    "        \n",
    "        # 自适应池化\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "print(\"🏗️  创建CNN模型...\")\n",
    "model = CNN(num_classes=8)\n",
    "print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"\\n📋 模型结构:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955bcb5-3a2d-474f-92ad-a3d3deee0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        with torch.no_grad():  \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"验证一个epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
    "    \"\"\"绘制训练历史\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 损失曲线\n",
    "    ax1.plot(train_losses, label='Training Loss', color='blue')\n",
    "    ax1.plot(val_losses, label='Validation Loss', color='red')\n",
    "    ax1.set_title('Loss vs Epoch')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 准确率曲线\n",
    "    ax2.plot(train_accs, label='Training Accuracy', color='blue')\n",
    "    ax2.plot(val_accs, label='Validation Accuracy', color='red')\n",
    "    ax2.set_title('Accuracy vs Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"早停机制\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a40a9-1378-4178-bd73-385103a08096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                num_epochs=50, device='cuda', patience=10):\n",
    "    \"\"\"完整的训练流程\"\"\"\n",
    "    \n",
    "    print(f\"🚀 开始训练，共 {num_epochs} 个epoch...\")\n",
    "    \n",
    "    # 初始化记录\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # 早停机制\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # 训练\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # 验证\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # 调整学习率\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # 打印结果\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_music_model.pth')\n",
    "            print(f\"🎯 新的最佳验证准确率: {best_val_acc:.2f}%\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"⏹️  早停触发，在第 {epoch+1} 个epoch停止训练\")\n",
    "            break\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n✅ 训练完成！\")\n",
    "    print(f\"⏱️  训练时间: {training_time/60:.2f} 分钟\")\n",
    "    print(f\"🏆 最佳验证准确率: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87071f3a-0f05-4ed7-bc69-403fcb79f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 优化器 - 使用经典配置\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=0.001,           # 标准学习率\n",
    "    weight_decay=1e-4   # L2正则化\n",
    ")\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 1.0)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "num_workers = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,           \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True         # 加速GPU传输\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,          \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"准备基线模型\")\n",
    "\n",
    "train_losses, val_losses, train_accs, val_accs = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66846a-e0cc-43f6-844d-5ac153d240ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "class ConfigurableCNN(nn.Module):\n",
    "    \"\"\"可配置的CNN模型，用于超参数调优\"\"\"\n",
    "    \n",
    "    def __init__(self, trial, num_classes=8):\n",
    "        super(ConfigurableCNN, self).__init__()\n",
    "        \n",
    "        # 通过trial对象获取超参数\n",
    "        self.n_layers = trial.suggest_int('n_layers', 2, 5)\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = 1\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            # 每层的通道数\n",
    "            out_channels = trial.suggest_categorical(f'n_units_l{i}', [16, 32, 64, 128, 256])\n",
    "            \n",
    "            # 卷积核大小\n",
    "            kernel_size = trial.suggest_categorical(f'kernel_size_l{i}', [3, 5])\n",
    "            \n",
    "            # Dropout概率\n",
    "            dropout_rate = trial.suggest_float(f'dropout_l{i}', 0.1, 0.5)\n",
    "            \n",
    "            # 构建卷积块\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                nn.Dropout2d(dropout_rate)\n",
    "            ])\n",
    "            \n",
    "            in_channels = out_channels\n",
    "        \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        # 自适应池化\n",
    "        pool_size = trial.suggest_categorical('pool_size', [2, 4, 8])\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((pool_size, pool_size))\n",
    "        \n",
    "        # 分类器\n",
    "        fc_input_size = in_channels * pool_size * pool_size\n",
    "        hidden_size = trial.suggest_categorical('fc_hidden_size', [128, 256, 512])\n",
    "        final_dropout = trial.suggest_float('final_dropout', 0.2, 0.7)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(fc_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(final_dropout * 0.5),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85943ebd-9b3c-4822-83a8-17de8a35713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optuna的目标函数\"\"\"\n",
    "    \n",
    "    # 1. 模型超参数\n",
    "    model = ConfigurableCNN(trial, num_classes=8).to(device)\n",
    "    \n",
    "    # 2. 训练超参数\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    \n",
    "    # 3. 优化器选择\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    \n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:  # SGD\n",
    "        momentum = trial.suggest_float('momentum', 0.8, 0.99)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "    \n",
    "    # 4. 创建数据加载器（使用新的batch_size）\n",
    "    if 'train_dataset' in globals():\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    else:\n",
    "        # 如果数据集不存在，返回一个虚拟值用于演示\n",
    "        return 0.5\n",
    "    \n",
    "    # 5. 训练配置\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    n_epochs = 10  # 为了快速调优，减少epoch数\n",
    "    \n",
    "    # 6. 训练循环\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # 训练\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if batch_idx > 50:  # 限制每个epoch的批次数，加速调优\n",
    "                break\n",
    "                \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                if batch_idx > 20:  # 限制验证批次数\n",
    "                    break\n",
    "                    \n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        \n",
    "        # 报告中间结果给Optuna\n",
    "        trial.report(val_acc, epoch)\n",
    "        \n",
    "        # 如果效果不好，提前剪枝\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return best_val_acc\n",
    "\n",
    "def run_hyperparameter_tuning(n_trials=50):\n",
    "    \"\"\"运行超参数调优\"\"\"\n",
    "    \n",
    "    print(\"🔍 开始超参数自动调优...\")\n",
    "    print(f\"将尝试 {n_trials} 种不同的超参数组合\")\n",
    "    \n",
    "    # 创建研究对象\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',  # 最大化验证准确率\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    "    )\n",
    "    \n",
    "    # 开始优化\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # 输出结果\n",
    "    print(\"🎯 超参数调优完成！\")\n",
    "    print(f\"最佳验证准确率: {study.best_value:.4f}\")\n",
    "    print(\"最佳超参数组合:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dacd19a-69fc-44f7-948b-fbbcccd31646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_grid_search():\n",
    "    \"\"\"手动网格搜索（更简单的调参方法）\"\"\"\n",
    "    \n",
    "    if 'train_dataset' not in globals():\n",
    "        print(\"⚠️  请先运行数据加载步骤！\")\n",
    "        return None\n",
    "    \n",
    "    print(\"🔍 开始手动网格搜索...\")\n",
    "    \n",
    "    # 定义搜索空间\n",
    "    param_grid = {\n",
    "        'lr': [0.001, 0.0005, 0.0001],\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'weight_decay': [1e-4, 1e-5, 1e-6],\n",
    "        'dropout_rate': [0.3, 0.4, 0.5]\n",
    "    }\n",
    "    \n",
    "    best_params = None\n",
    "    best_acc = 0.0\n",
    "    results = []\n",
    "    \n",
    "    # 遍历所有参数组合\n",
    "    from itertools import product\n",
    "    \n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    total_combinations = len(param_combinations)\n",
    "    \n",
    "    print(f\"总共需要测试 {total_combinations} 种参数组合\")\n",
    "    \n",
    "    for i, params in enumerate(param_combinations):\n",
    "        lr, batch_size, weight_decay, dropout_rate = params\n",
    "        \n",
    "        print(f\"\\n测试组合 {i+1}/{total_combinations}:\")\n",
    "        print(f\"  lr={lr}, batch_size={batch_size}, weight_decay={weight_decay}, dropout={dropout_rate}\")\n",
    "        \n",
    "        # 创建模型\n",
    "        model = DeepMusicCNN(num_classes=8).to(device)\n",
    "        \n",
    "        # 修改dropout率（这里简化处理）\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Dropout) or isinstance(module, nn.Dropout2d):\n",
    "                module.p = dropout_rate\n",
    "        \n",
    "        # 创建优化器\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # 快速训练（只训练几个epoch）\n",
    "        best_val_acc = 0.0\n",
    "        for epoch in range(5):  # 只训练5个epoch\n",
    "            # 训练\n",
    "            model.train()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                if batch_idx > 30:  # 限制批次数\n",
    "                    break\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # 验证\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                    if batch_idx > 15:  # 限制验证批次数\n",
    "                        break\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    total += target.size(0)\n",
    "                    correct += (predicted == target).sum().item()\n",
    "            \n",
    "            val_acc = correct / total\n",
    "            best_val_acc = max(best_val_acc, val_acc)\n",
    "        \n",
    "        results.append({\n",
    "            'params': {'lr': lr, 'batch_size': batch_size, 'weight_decay': weight_decay, 'dropout': dropout_rate},\n",
    "            'accuracy': best_val_acc\n",
    "        })\n",
    "        \n",
    "        print(f\"  最佳验证准确率: {best_val_acc:.4f}\")\n",
    "        \n",
    "        if best_val_acc > best_acc:\n",
    "            best_acc = best_val_acc\n",
    "            best_params = {'lr': lr, 'batch_size': batch_size, 'weight_decay': weight_decay, 'dropout': dropout_rate}\n",
    "            print(f\"  🎯 发现更好的参数组合！\")\n",
    "    \n",
    "    print(f\"\\n✅ 网格搜索完成！\")\n",
    "    print(f\"最佳准确率: {best_acc:.4f}\")\n",
    "    print(\"最佳参数:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return best_params, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc13dbf-c36f-4aad-a3fc-acd6e8962be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hyperparameter_tuning(n_trials=20)\n",
    "\n",
    "manual_grid_search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
